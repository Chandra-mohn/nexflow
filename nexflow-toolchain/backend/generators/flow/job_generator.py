"""
Job Generator Mixin

Generates the Flink job main class that orchestrates the entire pipeline.

COVENANT REFERENCE: See docs/COVENANT-Code-Generation-Principles.md
─────────────────────────────────────────────────────────────────────
This generator MUST follow the Zero-Code Imperative:
- NO STUBS: Every operator must wire to complete L3/L4 implementations
- NO TODOS: Generated code must be production-ready
- NO PLACEHOLDERS: All returns must have actual values

L1 is the "railroad" - it orchestrates but does NOT contain business logic.
Business logic lives in L3 (transforms) and L4 (rules).
─────────────────────────────────────────────────────────────────────
"""

from typing import Set

from backend.ast import proc_ast as ast
from backend.generators.common.java_utils import (
    to_pascal_case, to_camel_case, duration_to_ms
)
from backend.generators.flow.job_imports import JobImportsMixin
from backend.generators.flow.job_operators import JobOperatorsMixin
from backend.generators.flow.job_correlation import JobCorrelationMixin
from backend.generators.flow.job_sinks import JobSinksMixin


class JobGeneratorMixin(
    JobImportsMixin,
    JobOperatorsMixin,
    JobCorrelationMixin,
    JobSinksMixin
):
    """
    Mixin for generating Flink job main class.

    Generates:
    - Main class with StreamExecutionEnvironment setup
    - Pipeline construction from sources through sinks
    - Job execution call
    """

    def generate_job_class(self, process: ast.ProcessDefinition, package: str) -> str:
        """Generate complete Flink job main class."""
        class_name = to_pascal_case(process.name) + "Job"

        lines = [
            self._generate_file_header(class_name, process.name),
            f"package {package};",
            "",
            self._generate_job_imports(process),
            "",
            f"public class {class_name} {{",
            "",
            self._generate_constants(process),
            "",
            self._generate_main_method(process, class_name),
            "",
            self._generate_build_pipeline_method(process),
            "",
            "}",
        ]

        return '\n'.join(lines)

    def _generate_file_header(self, class_name: str, process_name: str) -> str:
        """Generate Java file header."""
        return f'''/**
 * {class_name}
 *
 * Flink Streaming Job for process: {process_name}
 *
 * AUTO-GENERATED by Nexflow Code Generator
 * DO NOT EDIT - Changes will be overwritten
 */'''

    def _generate_constants(self, process: ast.ProcessDefinition) -> str:
        """Generate job constants."""
        parallelism = process.execution.parallelism if process.execution else 4

        lines = [
            "    // Job Configuration",
            f"    private static final String JOB_NAME = \"{process.name}\";",
            f"    private static final int DEFAULT_PARALLELISM = {parallelism};",
            "    private static final String KAFKA_BOOTSTRAP_SERVERS = ",
            "        System.getenv().getOrDefault(\"KAFKA_BOOTSTRAP_SERVERS\", \"localhost:9092\");",
        ]

        return '\n'.join(lines)

    def _generate_main_method(self, process: ast.ProcessDefinition, class_name: str) -> str:
        """Generate main method."""
        return f'''    public static void main(String[] args) throws Exception {{
        // Create execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(DEFAULT_PARALLELISM);

        // Build and execute pipeline
        {class_name} job = new {class_name}();
        job.buildPipeline(env);

        // Execute
        env.execute(JOB_NAME);
    }}'''

    def _generate_build_pipeline_method(self, process: ast.ProcessDefinition) -> str:
        """Generate the buildPipeline method that constructs the streaming DAG."""
        lines = [
            "    /**",
            "     * Build the streaming pipeline.",
            "     */",
            "    public void buildPipeline(StreamExecutionEnvironment env) {",
        ]

        # Generate checkpoint configuration FIRST
        if process.resilience and process.resilience.checkpoint:
            lines.append(self._generate_checkpoint_setup(process.resilience.checkpoint))

        # Track current stream variable and type
        current_stream = None
        current_type = None

        # Generate source connections
        if process.input and process.input.receives:
            for receive in process.input.receives:
                source_code, stream_var, schema_class = self._generate_source_with_json(
                    receive, process
                )
                lines.append(source_code)
                current_stream = stream_var
                current_type = schema_class

        # Generate processing pipeline
        if process.processing:
            lines.append("")
            lines.append("        // Processing Pipeline")
            for idx, op in enumerate(process.processing):
                op_code, new_stream, new_type = self._wire_operator(
                    op, current_stream, current_type, idx
                )
                if op_code:
                    lines.append(op_code)
                    current_stream = new_stream
                    current_type = new_type

        # Generate correlation block
        if process.correlation:
            lines.append("")
            lines.append("        // Correlation Block")
            corr_code, new_stream, new_type = self._wire_correlation(
                process.correlation, process, current_stream, current_type
            )
            lines.append(corr_code)
            current_stream = new_stream
            current_type = new_type

        # Generate sinks
        if process.output and process.output.outputs:
            lines.append("")
            lines.append("        // Sinks")
            for output in process.output.outputs:
                if isinstance(output, ast.EmitDecl):
                    sink_code = self._generate_sink_with_json(
                        output, current_stream, current_type
                    )
                    lines.append(sink_code)

        # Generate late data sinks
        if process.processing:
            late_data_sinks = self._generate_late_data_sinks(process)
            if late_data_sinks:
                lines.append("")
                lines.append("        // Late Data Sinks")
                lines.extend(late_data_sinks)

        # Generate completion event sinks
        if process.completion:
            lines.append("")
            lines.append("        // Completion Event Callbacks")
            completion_code = self._wire_completion_block(
                process.completion, current_stream, current_type
            )
            lines.append(completion_code)

        lines.append("    }")

        return '\n'.join(lines)

    def _generate_source_with_json(
        self, receive: ast.ReceiveDecl, process: ast.ProcessDefinition
    ) -> tuple:
        """Generate source connection code with JSON deserialization."""
        source_name = receive.source
        source_var = to_camel_case(source_name) + "Source"
        schema_class = self._get_schema_class(receive)
        stream_var = to_camel_case(source_name) + "Stream"

        watermark_delay = "5000"
        if process.execution and process.execution.time and process.execution.time.watermark:
            watermark_delay = str(duration_to_ms(process.execution.time.watermark.delay))

        code = f'''
        // Source: {source_name}
        KafkaSource<{schema_class}> {source_var} = KafkaSource
            .<{schema_class}>builder()
            .setBootstrapServers(KAFKA_BOOTSTRAP_SERVERS)
            .setTopics("{source_name}")
            .setGroupId("{process.name}-consumer")
            .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST))
            .setValueOnlyDeserializer(new JsonDeserializationSchema<>({schema_class}.class))
            .build();

        DataStream<{schema_class}> {stream_var} = env
            .fromSource(
                {source_var},
                WatermarkStrategy.<{schema_class}>forBoundedOutOfOrderness(Duration.ofMillis({watermark_delay})),
                "{source_name}"
            );
'''
        return code, stream_var, schema_class

    def _generate_checkpoint_setup(self, checkpoint: ast.CheckpointBlock) -> str:
        """Generate checkpoint configuration."""
        interval_ms = duration_to_ms(checkpoint.interval)
        storage = checkpoint.storage

        return f'''
        // Checkpoint Configuration
        env.enableCheckpointing({interval_ms}, CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setCheckpointStorage("{storage}");
'''

    def _get_schema_class(self, receive: ast.ReceiveDecl) -> str:
        """Get schema class name from receive declaration."""
        if receive.schema and receive.schema.schema_name:
            return to_pascal_case(receive.schema.schema_name)
        return to_pascal_case(receive.source)

    def get_job_imports(self) -> Set[str]:
        """Get all imports needed for the job class."""
        return {
            'org.apache.flink.streaming.api.environment.StreamExecutionEnvironment',
            'org.apache.flink.streaming.api.datastream.DataStream',
            'org.apache.flink.api.common.eventtime.WatermarkStrategy',
            'org.apache.flink.connector.kafka.source.KafkaSource',
            'org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer',
            'org.apache.flink.connector.kafka.sink.KafkaSink',
            'org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema',
            'org.apache.flink.connector.base.DeliveryGuarantee',
            'org.apache.flink.streaming.api.CheckpointingMode',
            'org.apache.kafka.clients.consumer.OffsetResetStrategy',
            'java.time.Duration',
        }
